{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e0e981-42cc-42aa-b49e-40cc36b24c00",
   "metadata": {},
   "source": [
    "### Baseline Batch Inversion Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd98374-7f8a-4eea-9fb7-b7c90bf31170",
   "metadata": {},
   "source": [
    "- Run the script as is making sure you can step through an example and create plots\n",
    "\n",
    "- See if you can set errors and/or choose number of constraining observations to make estimate:\n",
    "\n",
    "    - As good as possible (tight boxplot/confidence bounds)\n",
    "    - As poor as possible (loose boxplot/confidence bounds)\n",
    "\n",
    "- Describe the above estimates:\n",
    "\n",
    "    - Monthly and annual flux estimates for oceans vs land, how are they different?\n",
    "\n",
    "    - Which land regions appear more difficult to constrain with limited global observations?\n",
    "\n",
    "- Using knowledge from above\n",
    "    - about how much of these 1,100,000 observations would you think you need to reasonably constrain most of the land flux regions?\n",
    "\n",
    "    - How much data do you think you’d need to simply constrain the global annual CO2 flux (all regions summed together) ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d90710-e75b-48b4-a48d-2715fa258fc7",
   "metadata": {},
   "source": [
    "#### Setting up Environment for Computing\n",
    "This cell reads from site_settings.yml file (top directory of ssim-ghg repos) and sets up environment, including directory references and libraries. Many more libraries and code scripts are read via the setup() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193a69a-e180-41a1-afb1-795d14b2124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#-- Read settings for location of data and set up, NO NEED TO CHANGE\n",
    "######################################################################\n",
    "orig_dir = getwd()\n",
    "require(yaml,warn.conflicts = FALSE)\n",
    "dat = yaml.load_file(\"../site_settings.yml\")\n",
    "Rcode_dir <- getwd()\n",
    "data_dir = paste(dat$global_paths$input_folder,\"/\",sep=\"\")\n",
    "output_dir = paste(dat$global_paths$output_folder,\"/\",sep=\"\")\n",
    "\n",
    "print(paste(\"Using\",data_dir,\"for data directory\"))\n",
    "print(paste(\"Using\",output_dir,\"for output directory\"))\n",
    "print(paste(\"Using\",Rcode_dir,\"for RCode dir\"))\n",
    "\n",
    "#--  Load utility code file w/ setup()\n",
    "source(file.path(Rcode_dir,\"util_code_032024.R\"))\n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d786e-c2c6-4eb1-a26f-7aaf026f37cc",
   "metadata": {},
   "source": [
    "#### Solving the equation\n",
    "Recall here we are simply solving the below equation, we therefore need inputs for each variable\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "J(x) = \\transpose{(x_0 - x)} {S_x\n",
    "}^{-1}(x_0 - x) + \\transpose{(z - Hx)} {S_z}^{-1}(z - Hx)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "\\hat{x} = (\\transpose{H}{S_z}^{-1}H + {S_x}^{-1})^{-1}(\\transpose{H}{S_z}^{-1}(z-Hx)+{S_x}^{-1}x_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "S_{\\hat{x}} = {({S_x}^{-1} + \\transpose{H}{S_z}^{-1}H )}^{-1}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb03a4a-39e9-461f-8c60-080b5a2f0532",
   "metadata": {},
   "source": [
    "#### Baseline Sensitivity Matrices (H and H^t)\n",
    "\n",
    "These precalculated sensitivity matrices (jacob object) detail the sensitivity of 1,156,383 different observations to the basis functions, which consist of 22 regions, 11 land and 11 ocean, as well as 24 months. The jacob_bgd object consists of the sensitivity of the observations to emission sources which will not be optimized here, particularly fire emissions (e.g. forest/grassland fires) and fossil fuel emissions.  At end we assign these objects to 'H' to match the notation through rest of exercises/slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab410eb9-f60e-4f6e-b365-af09c5be31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#--  Load sensitivity matrices \n",
    "###############################################\n",
    "\n",
    "load(file.path(data_dir,\"jacobians/\",\"trunc_full_jacob_030624_with_dimnames_sib4_4x5_mask.rda\"))\n",
    "load(file.path(data_dir,\"jacobians/\",\"jacob_bgd_060524.rda\"))\n",
    "\n",
    "#-- Difference in forward runs from GEOS-CHem resulted in CO2 vs C diff in mass is why 12/44 is here (note)\n",
    "#-- Assign the jacob objects to H to match notation\n",
    "H <- jacob * 12/44\n",
    "H_bgd <- jacob_bgd \n",
    "rm(jacob);rm(jacob_bgd)\n",
    "\n",
    "#-- These represent the fossil and biomass burning contributions to the observations (from fixed emission runs)\n",
    "fire_fixed <- H_bgd[,2]\n",
    "fossil_fixed <- H_bgd[,3]\n",
    "###################################################################\n",
    "#-- END END END ***Parent Directory and code for ALL inversions***\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e4905-6841-4089-b365-4508138f8c17",
   "metadata": {},
   "source": [
    "#### Set the \"truth\"\n",
    "\n",
    "This block of code sets up the (simulated) \"truth\" for the 528 element long state vector we described above. We've provided real life examples of what these can look like in the truth_array.  You can also simply set the state_vector_true to any vector of length 528.  Recall this state 'x' represents the adjustment to a baseline prior guess of fluxes such that the simulated true flux = 'prior best guess flux' * (1 + x).  This state will then be used to simulate our observations 'z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88569e4-d10b-4bb5-ba1b-a1d1f0e4f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################################\n",
    "#- Inversion #1   *************************\n",
    "##################################################################\n",
    "\n",
    "#################################\n",
    "#- Target truth in state space\n",
    "#################################\n",
    "\n",
    "##################################################################\n",
    "#-- This array holds ratios of OCO2v10MIP fluxes and SiB4 fluxes\n",
    "#-- as examples of \"scalings\" to be recovered. It also holds corresponding\n",
    "#-- differences if the inversion attempts to directly solve for flux\n",
    "#-- truth_array(24 months, 23 transcom, 98 inversions, (ratio, difference) )\n",
    "#-- To try another \"truth\" from these, just increment the third element below:\n",
    "#-- e.g. set * in xx = truth_array[,-1,*,1] to be between 1 and 98\n",
    "##################################################################\n",
    "\n",
    "#-- Don't Change\n",
    "#load(\"/projects/sandbox/inversion_workshop_scripts/truth_array.rda\")\n",
    "load(file.path(data_dir,\"misc/truth_array.rda\"))\n",
    "#-- pulling out NA transcom region and subset to scalar vs flux adj\n",
    "truth_array = truth_array[,-1,,1]\n",
    "#-- Don't Change\n",
    "\n",
    "\n",
    "#--  Choose our state from inversion list, option #1, and \"truncate\" to -1 and 1\n",
    "inversion_number =1   #  choose this between 1 and 98\n",
    "state_vector_true= tm(as.vector(- truth_array[,,inversion_number]),-1,1)\n",
    "\n",
    "#-- Alternatively choose a \"different\" true state like the below ones\n",
    "#-- The first just means the truth IS the prior, the second has a simple structure\n",
    "#-- Land regions fluxes are (1+0.5) * prior guess and ocean fluxes are (1- 0.5) * prior guess.\n",
    "#state_vector_true = c(rep(0,24*11),rep(0,24*11))\n",
    "#state_vector_true = c(rep(0.5,24*11),rep(-0.5,24*11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18954ba7-7890-4287-ac16-97c1a21a413c",
   "metadata": {},
   "source": [
    "#### Define the a priori flux covariance matrix\n",
    "Here we define what we are calling S_x, the a priori flux covariance matrix. In essence, this defines the bounds within which we expect to find our \"simulated\" truth, relative to the baseline best guess for prior flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544ec1d-253a-4c42-a33e-ac02f74bd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Generate a prior flux covariance matrix Sx\n",
    "# These first two lines form \"diagonal\" of Sx, e.g. marginal variances\n",
    "# Long term, a catalog of predefined choices is best here I think\n",
    "#########################################################\n",
    "land_prior_sd = 0.5   #-- free to set this, implies you think \"truth\" for land is within +/- 3*this\n",
    "ocean_prior_sd = 1    #-- free to set this, implies you think \"truth\" for ocean is within +/- 3*this\n",
    "\n",
    "##############################################################################\n",
    "#-- This is the structure of the 24 month subblock for each land/ocean region\n",
    "#-- induce temporal correlations\n",
    "##############################################################################\n",
    "\n",
    "#-- This will set up a prior temporal correlation, \n",
    "#-- free to set month_to_month_correlation between 0 (independent) and 1\n",
    "month_to_month_correlation = 0.5\n",
    "sigma = bdiag(rep(list(ar_covariance(24, month_to_month_correlation)), 22))  #-- free to set \n",
    "\n",
    "\n",
    "#################################################\n",
    "#-- scale by variance for land/ocean (set diagonal of matrix)\n",
    "#-- This simply puts together pieces above\n",
    "#################################################\n",
    "var_scaling_diagonal = diag(c(rep(land_prior_sd,24*11),rep(ocean_prior_sd,24*11)))\n",
    "\n",
    "Sx = as.matrix(var_scaling_diagonal %*% sigma %*% t(var_scaling_diagonal))\n",
    "\n",
    "#-- This is an alternative state_vector_true based *exactly* upon the prior covariance matrix\n",
    "#-- as opposed to being able to pick your \"truth\" separately from your assumed dist where \"truth\" lives\n",
    "#-- Probably don't want to change this unless you know what you are doing\n",
    "#state_vector_true = t(rmvnorm(n=1,mean=rep(0,528),sigma=sigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e423c95-d23d-49f9-9d25-aa3747667ae4",
   "metadata": {},
   "source": [
    "#### Choose which observations you want to assimilate\n",
    "Or in other words, which observations will be used to optimize/estimate the unknown fluxes.  This problem is somewhat over determined with over a million observations to constrain a 528 element state.  With that in mind, small observation errors and LOTS of observations used should \"nail the unknown\" solution quite well. The goal here is to create a vector of TRUE/FALSE of length equal to the total number of observations described in the sensitivity matrix we loaded above ( 1156383 ). The obs_catalog is a data.frame (think matrix of 'items'), with information about each observation and can be used to build a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d1ee9-5842-4e8f-96a7-eb5dfc329d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#-- WHICH obs do you want to use in the inversion? \n",
    "#-- examples of selecting on stations, type of data, lat/lon box,etc\n",
    "#-- essentially the \"subset_indicator_obs\" object is a vector of logicals (T/F) the \n",
    "#-- same length as the number of possible observations, i.e. 1156383\n",
    "####################################################################################\n",
    "\n",
    "load(file.path(data_dir,\"obs/obs_catalog_042424_unit_pulse_hour_timestamp_witherrors_withdates.rda\")) \n",
    "\n",
    "############################\n",
    "#-- Example 0: USE ALL OBS\n",
    "############################\n",
    "subset_indicator_obs=rep(TRUE,dim(H)[1])\n",
    "\n",
    "############################\n",
    "#-- SAMPLE BY TYPE EXAMPLE\n",
    "############################\n",
    "#subset_indicator_obs = obs_catalog$TYPE == \"TCCON\"\n",
    "#subset_indicator_obs = obs_catalog$TYPE == \"OCO2\"\n",
    "#subset_indicator_obs = obs_catalog$TYPE == \"IS\"\n",
    "\n",
    "###################################################################\n",
    "#-- Example 1: SAMPLE BY NOAA STATION EXAMPLE: Just use Mauna Loa 2 week flasks\n",
    "####################################################################\n",
    "# subset_indicator_obs = (\n",
    "#   grepl(\"mlo_surface-flask\", obs_catalog$ID)  #, add another via : | grepl(\"lef\", obs_catalog$ID)\n",
    "# )\n",
    "\n",
    "#####################################################################\n",
    "#-- Example 2: SAMPLE BY TIME EXAMPLE, THIS IS ONLY FLASK DATE FOR Aug 2015\n",
    "#####################################################################\n",
    "# subset_indicator_obs=(\n",
    "#   obs_catalog$YEAR == 2015\n",
    "#   & obs_catalog$MONTH == 8\n",
    "#    & obs_catalog$TYPE==\"IS\"\n",
    "#    &  grepl(\"flask\", obs_catalog$ID)\n",
    "# )\n",
    "\n",
    "##############################################################################\n",
    "#-- Example 3:  SAMPLE BY LON & LAT EXAMPLE \n",
    "#-- Data within 3deg of Equator and S. Hemisphere data cases\n",
    "##############################################################################\n",
    "\n",
    "#-- Southern Hemi data\n",
    "# subset_indicator_obs=(\n",
    "#   obs_catalog$LAT > -90\n",
    "#   & obs_catalog$LAT < -30   \n",
    "# )\n",
    "\n",
    "#-- Equator \"band\" of OCO2 XCO2 data\n",
    "# subset_indicator_obs=(\n",
    "#   obs_catalog$LAT > -3\n",
    "#   & obs_catalog$LAT < 3\n",
    "#   & obs_catalog$TYPE==\"OCO2\"\n",
    "# )\n",
    "\n",
    "############################\n",
    "#-- USE SIMPLE SUBSET\n",
    "############################\n",
    "#subset_size = 40000\n",
    "#subset_indicator_obs=rep(FALSE,dim(H)[1])\n",
    "#subset_indicator_obs[seq(1,1156383,length=subset_size)] = TRUE\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#-- Downsample if necessary to 578191 obs, watch running out of RAM\n",
    "#######################################################################\n",
    "\n",
    "#if(sum(subset_indicator_obs) > 0.5*length(subset_indicator_obs)) {\n",
    "#  new_ind = rep(FALSE,length(subset_indicator_obs))\n",
    "#  new_ind[sample(x=grep(TRUE,subset_indicator_obs),size=floor(0.5*length(subset_indicator_obs)))] = TRUE\n",
    "#  print(paste(\"downsampling from\",sum(subset_indicator_obs),\"to\",\n",
    "#              floor(0.5*length(subset_indicator_obs)),\"observations\"))\n",
    "#  subset_indicator_obs = new_ind\n",
    "#    }\n",
    "\n",
    "#-- LEAVE THIS AS IT SUMMARIZES THE NUMBER OF OBS USED\n",
    "print(paste(\"using\",sum(subset_indicator_obs),\"of\",length(subset_indicator_obs),\"observations\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5dcc37-f3e2-430a-a7ce-a16fc92f43d9",
   "metadata": {},
   "source": [
    "#### Set the observation errors\n",
    "Recall this component, matrix Sz, consists of the sum of (assumed) independent errors describing instrument noise and various transport errors due to representation and aggregation. You can simply set this error to be the same across all observations or use realistic errors as given in the obs_catalog object (from the OCO2MIP project). Note we don't allow off-diagonal non-zero entries here so we're carrying this matrix forward as vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e52525-cbb5-424b-8dc7-6a7a1a0dffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#-- sd for Gaussian i.i.d. errors, jacob is sens matrix\n",
    "##########################################################\n",
    "\n",
    "#-- Simple errors \n",
    "Sz_diagonal_in = rep(1,(dim(H)[1]))  # dim(H)[1] is length of obs possible\n",
    "\n",
    "#-- More realistic errors, \"real\" errors we use for these observation sites in \"real\" data inversion\n",
    "#Sz_diagonal_in = obs_catalog$SD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625eb554-7023-4394-abf6-0159e6a994d7",
   "metadata": {},
   "source": [
    "#### Simulate the true observations from the sensitivity matrix and the assumed observation errors\n",
    "Here we literally take the sensitivity matrix, our \"true\" state and the prior guess (the 1 in the calc below) and add our expected errors (Sz) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db9f7d-5129-48b0-b5e0-39ac59a300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#-- Generate obs, 'y',  set.seed() ????\n",
    "#-- currently leaving out bgd and all fixed\n",
    "#-- non-optimizable contributions including fire and fossil\n",
    "#############################################################\n",
    "\n",
    "z_in = H %*% (1+state_vector_true) + rnorm(length(Sz_diagonal_in),sd=Sz_diagonal_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25277386-ddbe-4d90-8f1e-5740a911a0d7",
   "metadata": {},
   "source": [
    "### The \"calculations\"\n",
    "Now we have every component defined and we simply do the calculations....\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "J(x) = \\transpose{(x_0 - x)} {\\Sigma_x\n",
    "}^{-1}(x_0 - x) + \\transpose{(z - Hx)} {\\Sigma_z}^{-1}(z - Hx)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "\\hat{x} = (\\transpose{H}{\\Sigma_z}^{-1}H + {\\Sigma_x}^{-1})^{-1}(\\transpose{H}{\\Sigma_z}^{-1}(z-Hx)+{\\Sigma_x}^{-1}x_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\transpose}[1]{{#1^{\\scriptscriptstyle T}}} \n",
    "\\Sigma_{\\hat{x}} = {({\\Sigma_x}^{-1} + \\transpose{H}{\\Sigma_z}^{-1}H )}^{-1}\n",
    "$$\n",
    "\n",
    "Actual baseline \"inversion\" code is now below...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4de4e-5572-4c49-93cd-d8aa155a3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#-- Run the actual inversion\n",
    "############################\n",
    "#-- Be aware DOF calc (DOF arg) and Kalman Gain  calc (output_Kalman_Gain) are a bit costly computationally\n",
    "#-- Try to leave DOF T or F, but output_Kalman_Gain=FALSE except in the kalman gain notebook example\n",
    "\n",
    "ret2  = invert_clean_notation(H=H,Sz_diagonal=Sz_diagonal_in,Sx=Sx,z=z_in,H_bgd=H_bgd,\n",
    "                    subset_indicator_obs=subset_indicator_obs,DOF=FALSE,output_Kalman_Gain=FALSE,\n",
    "                     state_vector_true=state_vector_true,force=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6eb2e-0d55-4b1a-8af6-23c42b7b497f",
   "metadata": {},
   "source": [
    "#### \"Sanity check\"\n",
    "The first sanity check here is to simply compare the predicted state with actual \"true\" state we defined above. If all is perfect, the points will line up on the 1:1 line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a9c1c-b3df-4957-bfd9-849f25c1b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist(ret2$posterior$x_hat[,1])\n",
    "options(repr.plot.width=8, repr.plot.height=8)\n",
    "plot(state_vector_true,ret2$posterior$x_hat,pch=16,cex=1.5,col=c(rep(\"orange\",264),rep(\"blue\",264)),\n",
    "     xlab=\"True State Scaling\",ylab=\"Estimated State Scaling\",main=\"Estimated state vector vs true state vector (all time and regions)\")\n",
    "lines(c(-100,100),c(-100,100),lty=1,lwd=3,col=\"grey\")\n",
    "legend(min(state_vector_true),max(ret2$posterior$x_hat),c(\"Land\",\"Ocean\"),pch=c(16,16),col=c(\"orange\",\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5129a-1767-4125-aeff-fa26f51fca1b",
   "metadata": {},
   "source": [
    "#### Maps of Flux, Prior, Post and Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00d5e1-bc07-47ee-bba4-7cb8804d3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flux_maps_annual_prior_post_truth=function (inv_object = ret2, true_state = state_vector_true, \n",
    "                                                 prior_mean_ncdf = file.path(data_dir, \"priors/prior_SiB4.nc\"), \n",
    "                                                 center_prior_on_zero = TRUE) \n",
    "{\n",
    "  print(\"creating gridded fluxes....\")\n",
    "  con = nc_open(prior_mean_ncdf)\n",
    "  longitude_prior = con$dim$longitude$vals\n",
    "  latitude_prior = con$dim$latitude$vals\n",
    "  NEE = ncvar_get(con, \"NEE\")\n",
    "  nc_close(con)\n",
    "  NEE_1x1 = aaply(NEE, 3, .fun = function(x) {\n",
    "    expand_5x4_2_1x1(x)\n",
    "  }) %>% aperm(c(2, 3, 1))\n",
    "  NEE_transcom = aaply(NEE_1x1, 3, .fun = function(x) {\n",
    "    grid2transcom(x)\n",
    "  })\n",
    "  tr_dir = file.path(data_dir, \"/transcom/\", sep = \"\")\n",
    "  x_prior_matrix = matrix(inv_object$prior$x_hat, nrow = 24, \n",
    "                          byrow = FALSE)\n",
    "  x_hat_matrix = matrix(inv_object$posterior$x_hat, nrow = 24, \n",
    "                        byrow = FALSE)\n",
    "  true_state_matrix = matrix(true_state, nrow = 24, byrow = FALSE)\n",
    "  prior_flux_unc = diag(as.vector(NEE_transcom[, 1:22])) %*% \n",
    "    inv_object$prior$Sx %*% diag(as.vector(NEE_transcom[, \n",
    "                                                        1:22]))\n",
    "  post_flux_unc = diag(as.vector(NEE_transcom[, 1:22])) %*% \n",
    "    inv_object$posterior$Sx %*% diag(as.vector(NEE_transcom[, \n",
    "                                                            1:22]))\n",
    "  A = cbind(diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), \n",
    "            diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), \n",
    "            diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), \n",
    "            diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22), diag(1, nrow = 22), diag(1, nrow = 22), diag(1,nrow = 22))\n",
    "  \n",
    "  annual_avg_prior_flux_cov = 0.5 * A %*% prior_flux_unc %*% \n",
    "    t(0.5 * A)\n",
    "  annual_avg_post_flux_cov = 0.5 * A %*% post_flux_unc %*% \n",
    "    t(0.5 * A)\n",
    "  gridded_1x1_prior_sd_flux_annual = transcom2grid(sqrt(diag(annual_avg_prior_flux_cov))) * \n",
    "    1e-15\n",
    "  gridded_1x1_post_sd_flux_annual = transcom2grid(sqrt(diag(annual_avg_post_flux_cov))) * \n",
    "    1e-15\n",
    "  gridded_1x1_post_sd_reduction_annual = 1 - (gridded_1x1_post_sd_flux_annual/gridded_1x1_prior_sd_flux_annual)\n",
    "  if (!center_prior_on_zero) {\n",
    "    x_prior_matrix = x_prior_matrix + 1\n",
    "    x_hat_matrix = x_hat_matrix + 1\n",
    "    true_state_matrix = true_state_matrix + 1\n",
    "  }\n",
    "  gridded_1x1_prior_state = aaply(x_prior_matrix, 1, .fun = function(x) {\n",
    "    transcom2grid(x, model.grid.x = 1, model.grid.y = 1, \n",
    "                  file_location = data_dir)\n",
    "  }) %>% aperm(c(2, 3, 1))\n",
    "  gridded_1x1_posterior_state = aaply(x_hat_matrix, 1, .fun = function(x) {\n",
    "    transcom2grid(x, model.grid.x = 1, model.grid.y = 1, \n",
    "                  file_location = data_dir)\n",
    "  }) %>% aperm(c(2, 3, 1))\n",
    "  gridded_1x1_true_state = aaply(true_state_matrix, 1, .fun = function(x) {\n",
    "    transcom2grid(x, model.grid.x = 1, model.grid.y = 1, \n",
    "                  file_location = data_dir)\n",
    "  }) %>% aperm(c(2, 3, 1))\n",
    "  gridded_1x1_prior_mean_flux = NEE_1x1 * gridded_1x1_prior_state\n",
    "  gridded_1x1_posterior_mean_flux = NEE_1x1 * gridded_1x1_posterior_state\n",
    "  gridded_1x1_truth = NEE_1x1 * gridded_1x1_true_state\n",
    "  gridded_1x1_prior_mean_flux_annual = apply(gridded_1x1_prior_mean_flux, \n",
    "                                             c(1, 2), sum)/2\n",
    "  gridded_1x1_posterior_mean_flux_annual = apply(gridded_1x1_posterior_mean_flux, \n",
    "                                                 c(1, 2), sum)/2\n",
    "  gridded_1x1_true_mean_flux_annual = apply(gridded_1x1_truth, \n",
    "                                            c(1, 2), sum)/2\n",
    "  library(maps)\n",
    "  w = map(\"world\", plot = FALSE)\n",
    "  grd = expand.grid(longitude = seq(-179.5, 179.5, by = 1), \n",
    "                    latitude = seq(-89.5, 89.5, by = 1))\n",
    "  units_scaling = 1000 * 12/44 * 3600 * 24 * 30.5\n",
    "  grd$prior.mean = as.vector(gridded_1x1_prior_mean_flux_annual) * \n",
    "    units_scaling\n",
    "  grd$posterior.mean = as.vector(gridded_1x1_posterior_mean_flux_annual) * \n",
    "    units_scaling\n",
    "  grd$truth = as.vector(gridded_1x1_true_mean_flux_annual) * \n",
    "    units_scaling\n",
    "  grd$difference = grd$posterior.mean - grd$truth\n",
    "  grd$prior_sd = as.vector(gridded_1x1_prior_sd_flux_annual) * \n",
    "    units_scaling\n",
    "  grd$post_sd = as.vector(gridded_1x1_post_sd_flux_annual) * \n",
    "    units_scaling\n",
    "  grd$reduction_sd = as.vector(gridded_1x1_post_sd_reduction_annual)\n",
    "  rng_mn = range(c(grd$prior.mean, grd$posterior.mean, grd$truth, \n",
    "                   grd$difference))\n",
    "  rng_sd = range(c(grd$prior_sd, grd$post_sd))\n",
    "\n",
    "  library(ggplot2)\n",
    "  w2 =    as.data.frame(cbind(w$x,w$y))\n",
    "  names(w2) = c(\"x\",\"y\")\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  # Set color scale limits\n",
    "  lims_mn <- seq(-max(abs(c(rng_mn))), max(abs(c(rng_mn))), length.out = 50)\n",
    "  lims_sd <- seq(0, max(abs(c(rng_sd))), length.out = 50)\n",
    "  \n",
    "  plt1=ggplot(grd, aes(x = longitude, y = latitude, fill = prior.mean)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50),\n",
    "      limits = range(lims_mn),\n",
    "      name = \"gC/m2/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Annual Prior Mean Flux (gC/m2/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "  \n",
    "  plt2=ggplot(grd, aes(x = longitude, y = latitude, fill = posterior.mean)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50),\n",
    "      limits = range(lims_mn),\n",
    "      name = \"gC/m2/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Annual Posterior Mean Flux (gC/m2/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "  \n",
    "  plt3=ggplot(grd, aes(x = longitude, y = latitude, fill = truth)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50),\n",
    "      limits = range(lims_mn),\n",
    "      name = \"gC/m²/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"True Flux (gC/m2/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "  \n",
    "  plt4=ggplot(grd, aes(x = longitude, y = latitude, fill = difference)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50),\n",
    "      limits = range(lims_mn),\n",
    "      name = \"gC/m²/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Posterior Mean - Truth (gC/m2/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "\n",
    "    \n",
    "  plt5=ggplot(grd, aes(x = longitude, y = latitude, fill = prior_sd)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50)[26:50],\n",
    "      limits = range(lims_sd),\n",
    "      name = \"gC/m²/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Prior Standard Deviation (PgC/region/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "  \n",
    "  plt6=ggplot(grd, aes(x = longitude, y = latitude, fill = post_sd)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = my.col(50)[26:50],\n",
    "      limits = range(lims_sd),\n",
    "      name = \"gC/m²/yr\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Posterior Standard Deviation (PgC/region/yr)\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "  \n",
    "  plt7=ggplot(grd, aes(x = longitude, y = latitude, fill = reduction_sd)) +\n",
    "    geom_raster(interpolate = TRUE) +  # Or use geom_tile() if you want crisp edges\n",
    "    scale_fill_gradientn(\n",
    "      colours = rev(my.col(50)),\n",
    "      limits = range(lims_sd),\n",
    "      name = \"\"\n",
    "    ) +\n",
    "    geom_path(data = w2, aes(x = x, y = y), linewidth = 0.25, inherit.aes = FALSE, color = \"black\") +\n",
    "    coord_fixed() +\n",
    "    labs(title = \"Uncertainty Reduction: 1-Posterior_SD/Prior_SD\", x = \"\", y = \"\") +\n",
    "    theme_minimal()\n",
    "\n",
    "    \n",
    "  options(jupyter.plot_scale = 1)\n",
    "  options(repr.plot.width=10, repr.plot.height=5)\n",
    "  #plt = arrangeGrob(grobs=list(plt1,plt2))\n",
    "  #plot(plt1)\n",
    "  #plot(plt2,newpage=FALSE)\n",
    "  #plot(plt3,newpage=FALSE)\n",
    "  #plot(plt4,newpage=FALSE)  \n",
    "  #plt = arrangeGrob(grobs=list(plt3,plt4))\n",
    "  #plot(plt)\n",
    "  #plt = arrangeGrob(grobs=list(plt5,plt6))\n",
    "  #plot(plt)\n",
    "  #plt = arrangeGrob(grobs=list(plt7))\n",
    "  #plot(plt)\n",
    "  #saved <- options()\n",
    "   #options(repr.plot.width=8, repr.plot.height=8)\n",
    "  #plt = marrangeGrob(grobs=list(plt1,plt2,plt3,plt4),layout_matrix=matrix(1:4,ncol=1))\n",
    "  #plot(plt1)\n",
    "  #plot(plt2)\n",
    "#plot(plt3)\n",
    "  #options(saved)\n",
    "    return(list(plt1,plt2,plt3,plt4,plt5,plt6,plt7))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2701e0e-85f4-4895-b0d3-e83d6511bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flux_maps_annual_prior_post_truth(inv_object = ret2, true_state = state_vector_true, \n",
    "    prior_mean_ncdf = file.path(data_dir, \"priors/prior_SiB4.nc\"), \n",
    "    center_prior_on_zero = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290bef6-1d20-4905-922f-f2d6ed8a9ecf",
   "metadata": {},
   "source": [
    "#### Produce a Monte Carlo estimate from analytical inversion output in order to quickly plot results\n",
    "We could perfectly produce the following plots from analytical solutions (linear combinations of the different flux pieces in time and space) coming from inversion but choose to take a large sample of observations to facilitate quick and efficient plotting, in particular boxplots.  The first line returns a sample of fluxes that can be plotted (simply the perturbations to the prior that we are optimizing) while the second option adds the original prior mean fluxes for both the ocean and land.  Takes a little while to generate but run both lines if unsure of what to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb033cd5-1175-4f00-aba5-219f71b6cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = generate_transcom_flux_ensemble_from_inversion(inv_object=ret2,samples=1000)\n",
    "\n",
    "org_data_add_ocn_land_prior_means = generate_transcom_flux_ensemble_from_inversion(inv_object=ret2,samples=1000,include_ocn_land_prior=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d427d08-db2f-4074-b21a-6a683253a364",
   "metadata": {},
   "source": [
    "### Here we will plot annual flux average for 9/2014 - 8/2016 for each Transcom Region.\n",
    "***Very important, all these boxplots/confidence bounds plots will be on the deviation from the prior flux, i.e. H\\*state_vector_true and not H\\*(1+state_vector_true). These also represent the inversion estimate of the deviation from the prior with no fires or fossil fuel emissions added back in.  We *could* add the prior mean ocean and land fluxes into the boxplots below but on the annual scale (not monthly/seasonal) they are close to zero so it would be hard to even discern that they were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d69704-2dbc-4529-9bda-f0d554c0e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries_flux_bytranscom(ret=org_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3feb269-2cdc-4408-8068-e7cb72d30524",
   "metadata": {},
   "source": [
    "#### Plotting \"confidence bounds\" for monthly flux estimates by Transcom Region\n",
    "In this next section of code we will plot the posterior \"credible intervals\" for each transcom region as a function of month from 9/2014 to 8/2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861a331-8f5c-46e0-ac27-19ccae41b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot just the adjustment the inversion is making\n",
    "#plot_transcom_flux_by_month(ret=org_data)\n",
    "\n",
    "#Plot the adjustment PLUS the prior means for land and bio\n",
    "plot_transcom_flux_by_month(ret=org_data_add_ocn_land_prior_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b26301-9e6b-4cbb-9fdd-089f4e77bbe0",
   "metadata": {},
   "source": [
    "#### Plotting prior/posterior correlations across fluxes\n",
    "Here we are plotting prior/post correlation across 2 year flux average (then month by month for different regions in next code block). Note that correlations are estimated from samples in \"orig_data\" hence prior shows \"some\" correlation when none exists due to noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4307a-3c7f-4db1-82f0-c470e907bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inversion_correlations(org_data = org_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109f2df-cb5c-4679-8986-8569090208f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inversion_correlations_by_transcom(org_data=org_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62038a4f-e518-4e29-b073-18d222b2d1ac",
   "metadata": {},
   "source": [
    "#### Plot concentration time series at different sites\n",
    "When add_prior_nee=TRUE,add_fossil=FALSE,and add_fire=FALSE, only the inversion produced adjustments to the site level concentrations are plotted (note prior=0 then). add_prior_nee=TRUE adds the underlying best guess initial prior which is being scaled by the inversion to prior/posterior/obs, you can note the seasonal cycle in the prior *appearing*.  add_fossil=TRUE and add_fire=TRUE add fixed contributions from fossil fuels and biomass burning to all the concentrations resulting in more *realistic* concentration time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1387cb9-1d26-4e92-900d-83f5e9eb674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key default arg here which you can change: site_strings=c(\"brw\",\"mlo\",\"smo\",\"co2_spo_surface-flask\",\"lef\",\"wkt\",\"wbi\",\"nwr\",\"hun\")\n",
    "plot_concentrations(inversion=ret2,add_prior_nee=TRUE,add_fossil=FALSE,add_fire=FALSE,\n",
    "           site_strings=c(\"brw_surface-flask_1_representative\",\"mlo_surface-flask_1_representative\",\n",
    "                          \"smo_surface-flask_1_representative\",\"spo_surface-flask_1_representative\",\n",
    "                          \"lef\",\"wkt\",\"wbi\",\"nwr_surface-flask_1_representative\",\"hun\")        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8d4cf-5369-4ae9-a3b8-22f1f7fbb69b",
   "metadata": {},
   "source": [
    "#### Writing out the inversion \"object\" to a netcdf file so that anybody can read it\n",
    "You don't have to save out this object to netcdf file every time.  In fact, I wouldn't, it will take a few minutes for this to run to save to netcdf files. However, you *will* need it to make the standard deviation maps in next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcbeac-495c-4ceb-a6ee-87e59eee0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Write out \"posterior\" object\n",
    "system.time(write_inversion_2_netcdfs(inv_object=ret2,subobject=\"posterior\",\n",
    "                                      prior_mean_ncdf=file.path(data_dir,\"priors/prior_SiB4.nc\"),\n",
    "                                      sample_number=100,output_dir=output_dir))\n",
    "\n",
    "#-- Write out \"posterior\" object\n",
    "system.time(write_inversion_2_netcdfs(inv_object=ret2,subobject=\"prior\",\n",
    "                                      prior_mean_ncdf=file.path(data_dir,\"priors/prior_SiB4.nc\"),\n",
    "                                      sample_number=100,output_dir=output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469459a8-ee5e-44b9-9d9a-faed2fe8e2d3",
   "metadata": {},
   "source": [
    "#### Prior vs Posterior Marginal Standard Deviation and Uncertainty Reduction\n",
    "We need to have written out the above inversion objects to netcdf files before running the below plots. Sorry, this code takes just a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330af62-3d15-4000-b1f7-8aa009b8d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flux_maps_annual_prior_post(prior_file_nc=\"/Users/aschuh/temp/ssim-ghg-output/gridded_fluxes_prior.nc4\",\n",
    "                                            posterior_file_nc=\"/Users/aschuh/temp/ssim-ghg-output/gridded_fluxes_posterior.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd63c6a-1b2f-4a7e-b4b5-af207bbe3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()\n",
    "#show info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bb482-f580-4a1d-90a6-7c719137549d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
