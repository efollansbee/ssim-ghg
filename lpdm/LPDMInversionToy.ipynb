{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc1aaae-02be-4fab-b2dc-459ea976429f",
   "metadata": {},
   "source": [
    "Let's first setup our libraries before we head onto the inversion along with loading our lat/long grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06648ab6-9f28-4456-911e-490cfbe5b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Import Libraries\n",
    "import sys\n",
    "sys.path.append('C:/Users/klm3/AppData/Local/Programs/Python/Python311/Lib/site-packages')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.sparse import diags\n",
    "from scipy.linalg import inv as dense_inv\n",
    "from scipy.linalg import cholesky \n",
    "import scipy.sparse as sp\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Polygon, Point, box\n",
    "from matplotlib.colors import Normalize, LogNorm \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mask_df = pd.read_csv('G:/SummerSchool/shapefiles/mask.csv')\n",
    "mask_array = mask_df['Mask'].values\n",
    "num_fluxes =sum(mask_array==1)\n",
    " \n",
    "#add lons and lats\n",
    "subdir = 'priors_processed'\n",
    "priorstring = 'ACES_FFDAS'  \n",
    "lats = np.load('G:/SummerSchool/priors/'+subdir+'/'+priorstring+'_lat.npy')\n",
    "lons = np.load('G:/SummerSchool/priors/'+subdir+'/'+priorstring+'_long.npy')\n",
    "lons = lons[mask_array.ravel().astype(bool)] #removes rows where mask_array = 0\n",
    "lats = lats[mask_array.ravel().astype(bool)]    \n",
    "lat_grid = np.unique(lats)\n",
    "lon_grid = np.unique(lons)\n",
    "mask = True\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92216f6-cdf9-4bc1-bc52-15c9fbec8b5b",
   "metadata": {},
   "source": [
    "In this code base, you choose different options to run your inversion.  Essentially what you are looking at is using the code to explore different types of setups that can help you find the best setup for your situation.  Remember if the base case does not work (truth = prior, met is the same, no error on y, no bias, etc.) then you are doing something wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13af832-b8c7-464f-9791-19f52cea7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Create y for the Inversion\n",
    "#variables for the inversion\n",
    "priorslist = ['ACES_FFDAS', 'GRAAPESCO2']\n",
    "tower_names = ['NEB','NWB','HAL']\n",
    "monthlist = ['02','07']\n",
    "truth = 'ACES_FFDAS'  ### YOU CAN CHOOSE EITHER FOR YOUR TRUTH OR YOUR PRIOR\n",
    "prior = 'ACES_FFDAS'\n",
    "truemet = 'WRF2' #STICK WITH WRF2 FOR THIS TOY EXAMPLE\n",
    "met = truemet #DO NOT CHANGE THIS\n",
    "\n",
    "R_whitenoise = False #YOU HAVE THE OPTION OF USING WHITE NOISE HERE BUT THERE ARE OTHER WAYS TO SPECIFY THIS \n",
    "R_perfect = True #ONLY USE THIS TO ENSURE THAT YOUR INVERSION IS WORKING PERFECTLY\n",
    "y_bias = False #YOU CAN CHOOSE A BIAS TO INCLUDE ON YOUR \"REAL OBERVATIONS\" TO SEE HOW A BIAS WILL IMPACT YOUR RESULTS\n",
    "y_whitenoise = False #YOU CAN ADD HOW MUCH WHITENOISE TO USE\n",
    "bias = 0 #HERE IS WHERE YOU WILL SPECIFIFY THE BIAS\n",
    "signal = 1 #USED TO MAKE NOISE ON \"REAL OBSERVATIONS\" AND ON R IF R_WHITENOISE AND Y_WHITENOISE ARE TRUE\n",
    "R_param =  signal**2# THIS IS THE WHAT WILL CREATE THE NOISE IN PPM^2 SPACE OR BE A CONSTANT VARIANCE ACROSS ALL OBSERVATIONAL TIME PERIODS\n",
    "unc_save = False #LEAVE THIS AS FALSE\n",
    "q_floor = 1 #CREATES A FLOOR ON Q IF YOU DECIDE TO USE THE VALUE OF THE PRIORS ALONG THE DIAGONAL - THIS ENSURES THAT THE MATRICES CAN BE INVERTED\n",
    "qones = False #IF YOU WANT Q TO BE A CONSTANT DIAGONAL - SET FLAG TO TRUE.  IF YOU WANT TO VARY BY THE VALUES OF THE PRIOR SET FLAG TO FALSE\n",
    "if qones:\n",
    "    q_param = 1 #IF DIAGONAL YOU WANT TO SET THIS\n",
    "else:\n",
    "    q_param = 1 #KEEP THIS AS ONE IF YOU WANT TO HAVE DIAGONAL VARYING BY PRIOR\n",
    "z_directory = 'G:/SummerSchool/output/enhancements/'\n",
    "y_save_directory = 'G:/SummerSchool/output/y/'\n",
    "\n",
    "print('True met = ' + truemet)\n",
    "print('Prior met = ' + met)\n",
    "print('True emissions = ' + truth)\n",
    "print('Prior fluxes = '+ prior) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c6bf2-2bf0-4b2b-b0da-907738937237",
   "metadata": {},
   "source": [
    "In this block, we are loading data and seeing what the mean enhancement is for our \"observed enhancements\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ed06c-e720-4190-a252-11eff80e7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Data')\n",
    "def load_numpy_array(directory,fname):\n",
    "    filename = f'{directory}{fname}'\n",
    "    data = np.load(filename)\n",
    "    return data\n",
    "\n",
    "#load\n",
    "values_feb_truth = []\n",
    "values_feb_prior = []\n",
    "values_jul_truth = []\n",
    "values_jul_prior = []\n",
    "unit_WRF2_feb =[]\n",
    "unit_WRF2_jul = []\n",
    "\n",
    "for site in tower_names:\n",
    "    #Feb\n",
    "    filename_feb_prior = site + '_y_' + prior+'_02_2019.npy'\n",
    "    feb_values_prior = load_numpy_array(y_save_directory,filename_feb_prior)\n",
    "    values_feb_prior.append(feb_values_prior)\n",
    "    \n",
    "    filename_feb_truth = site + '_y_' + truth+'_02_2019.npy'\n",
    "    feb_values_truth = load_numpy_array(y_save_directory,filename_feb_truth)\n",
    "    values_feb_truth.append(feb_values_truth)\n",
    "    #print('y (feb)'+site + ' ' + str(len(feb_values_truth)))\n",
    "\n",
    "    u_WRF2_feb = np.load(z_directory + 'unit_'+ site+'_2019_02_WRF2.npy') #change later\n",
    "    u_WRF2_feb = [x for x in u_WRF2_feb if not pd.isna(x) and x!='nan']\n",
    "    u_WRF2_feb  = np.array(u_WRF2_feb)\n",
    "    unit_WRF2_feb.append(u_WRF2_feb) \n",
    "    \n",
    "    #Jul    \n",
    "    filename_jul_prior = site + '_y_' + prior+'_07_2019.npy'\n",
    "    jul_values_prior = load_numpy_array(y_save_directory,filename_jul_prior)\n",
    "    values_jul_prior.append(jul_values_prior)\n",
    "    \n",
    "    filename_jul_truth = site + '_y_' + truth+'_07_2019.npy'\n",
    "    jul_values_truth = load_numpy_array(y_save_directory,filename_jul_truth)\n",
    "    values_jul_truth.append(jul_values_truth)\n",
    "    \n",
    "    u_WRF2_jul = np.load(z_directory + 'unit_'+ site+'_2019_07_WRF2.npy') #change later\n",
    "    u_WRF2_jul = [x for x in u_WRF2_jul if not pd.isna(x) and x!='nan']\n",
    "    u_WRF2_jul  = np.array(u_WRF2_jul)\n",
    "    unit_WRF2_jul.append(u_WRF2_jul)\n",
    "\n",
    "#Feb\n",
    "y_feb_array_truth = np.concatenate(values_feb_truth)\n",
    "y_feb_array_prior = np.concatenate(values_feb_prior)\n",
    "r_unit_WRF2_feb = np.concatenate(unit_WRF2_feb)\n",
    "feb_mean = np.mean(y_feb_array_truth)\n",
    "#Jul\n",
    "y_jul_array_truth = np.concatenate(values_jul_truth)\n",
    "y_jul_array_prior = np.concatenate(values_jul_prior)\n",
    "r_unit_WRF2_jul = np.concatenate(unit_WRF2_jul)\n",
    "jul_mean = np.mean(y_jul_array_truth)\n",
    "\n",
    "print('Mean signal (ytruth) for Feb is '+ str(round(feb_mean,2)) +' ppm')\n",
    "print('Mean signal (ytruth) for Jul is '+ str(round(jul_mean,2)) + ' ppm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099eb22-0ea9-4427-86c7-a8461a95a005",
   "metadata": {},
   "source": [
    "Let's first create the R matrix and see how signal we have to noise.  You already specified how to create R so you don't need to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f457d0-c0c4-489f-bc10-14e38a858caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' ')\n",
    "if R_whitenoise:\n",
    "    noise_feb = np.random.normal(0,signal, size = y_feb_array_prior.shape)\n",
    "    noise_jul = np.random.normal(0,signal, size = y_jul_array_prior.shape)\n",
    "    R_Feb = noise_feb*R_param\n",
    "    R_Jul = noise_jul*R_param\n",
    "    print('R = diag of ones of multiplied by ' + str(round(R_param,2)) + ' ppm2 in Feb and Jul')\n",
    "else:\n",
    "    if R_perfect:\n",
    "        R_Feb = np.ones(y_feb_array_prior.shape)*0.1 #this is the perfect case\n",
    "        R_Jul = np.ones(y_jul_array_prior.shape)*0.1\n",
    "    else: \n",
    "        R_Feb = np.ones(y_feb_array_prior.shape)*R_param#this is constant R\n",
    "        R_Jul = np.ones(y_jul_array_prior.shape)*R_param\n",
    "\n",
    "#%% R noise and plot \n",
    "fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "ax[0].plot(y_feb_array_truth,label='y:truth',color='black',linewidth =.75)\n",
    "ax[0].plot(np.sqrt(R_Feb),label='R '+str(round(R_param,2))+'ppm',color='red',linewidth =1)\n",
    "ax[0].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[0].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[0].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[0].set_title(\"Truth Enh. & sqrt(R) (\" + prior+\"-\"+met+\") Feb 2019\", fontsize=14)\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(y_jul_array_truth,label='y:truth',color='black',linewidth =.75)\n",
    "ax[1].plot(R_Jul,label='R '+str(round(R_param,2))+'ppm',color='red',linewidth =1)\n",
    "ax[1].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[1].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[1].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[1].set_title(\"Truth Enh. & sqrt(R) (\" + prior+\"-\"+met+\") July 2019\", fontsize=14)\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.15)\n",
    "plt.show\n",
    "\n",
    "print(' ')\n",
    "print('R and y truth for Feb and July created!')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a365a-dc39-4966-8462-f4c645cecff8",
   "metadata": {},
   "source": [
    "What can you say about how you set up the problem?  How much signal is there to noise in R is there for Feb and July?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10efd64-03d3-43bf-86c2-1a76f0590af0",
   "metadata": {},
   "source": [
    "Now we are going to create Q and load our H matrices for February and July.  You always want to check your dimensions on everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d243fa-6b63-43b1-a05a-08dfa8c42be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Q & Loading Hmatrix\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    return sp.load_npz(filename)\n",
    "\n",
    "Hmatrix_feb = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_02.npz') \n",
    "Hmatrix_jul = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_07.npz') \n",
    "\n",
    "prior_array_feb = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_02.npy')\n",
    "prior_array_jul = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_07.npy')\n",
    "\n",
    "truth_array_feb = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_02.npy')\n",
    "truth_array_jul = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_07.npy')\n",
    "q_size_jul = len(prior_array_jul)\n",
    "q_size_feb = len(prior_array_feb)\n",
    "\n",
    "#Check dimensions\n",
    "print(f\"Hmatrix shape(Feb): {Hmatrix_feb.shape}\")    \n",
    "print('Prior length: ' + str(len(prior_array_feb)))\n",
    "print('Size of Q (July) = ' + str(q_size_jul) + ' x ' +str(q_size_jul))\n",
    "print('Mean prior val for Jul is '+ str(round(np.mean(prior_array_jul),2)) + ' umol/m2s')\n",
    "print('##')\n",
    "print(f\"Hmatrix shape(July): {Hmatrix_jul.shape}\")    \n",
    "print('Prior length: ' + str(len(prior_array_jul)))\n",
    "print('Mean signal (ytruth) for Feb is '+ str(round(np.mean(prior_array_feb),2)) +' umol/m2s')\n",
    "print('Size of Q (Feb) = ' + str(q_size_feb) + ' x ' +str(q_size_feb))\n",
    "\n",
    "Hsp_jul = Hmatrix_jul@prior_array_jul #should be the same as original y_jul_array_prior\n",
    "Hsp_feb = Hmatrix_feb@prior_array_feb #should be the same as original y_feb_array_prior\n",
    "print('Created Hxsp')\n",
    "\n",
    "y_feb_array_truth_hold = y_feb_array_truth.copy()\n",
    "y_jul_array_truth_hold = y_jul_array_truth.copy()\n",
    "if y_bias:\n",
    "    y_feb_array_truth = y_feb_array_truth+bias\n",
    "    y_jul_array_truth = y_jul_array_truth+bias\n",
    "if y_whitenoise:\n",
    "    y_feb_array_truth = y_feb_array_truth+noise_feb\n",
    "    y_jul_array_truth = y_jul_array_truth+noise_jul    \n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "ax[0].plot(y_feb_array_truth,label='y:true enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[0].plot(Hsp_feb,label='Hs:modelled enhan.',color='red',linewidth =1)\n",
    "ax[0].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[0].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[0].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[0].set_title(\"y truth vs Hs Feb 2019\", fontsize=14) \n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(y_jul_array_truth,label='y:true enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[1].plot(Hsp_jul,label='Hs:modelled enh.',color='red',linewidth =1)\n",
    "ax[1].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[1].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[1].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[1].set_title(\"y truth vs Hs Jul 2019\", fontsize=14)\n",
    "ax[1].grid(True)\n",
    "\n",
    "if qones:\n",
    "    print('Q is diag of ones * ' + str(q_param**2) + ' umol/m2s^2')\n",
    "    Q_diag_feb = np.ones(prior_array_feb.shape[0])*(np.square(q_param))\n",
    "    Q_diag_feb = diags(Q_diag_feb,0)\n",
    "    Q_diag_jul = np.ones(prior_array_jul.shape[0])*(np.square(q_param))\n",
    "    Q_diag_jul = diags(Q_diag_jul,0)\n",
    "else:\n",
    "    print('Q is varying (per prior values) with scaling factor of ' + str(q_param)) \n",
    "    q_prior_array_feb = prior_array_feb.copy()\n",
    "    q_prior_array_feb[q_prior_array_feb < 1] = q_floor\n",
    "    q_diag_feb = (q_param**2) * np.square(q_prior_array_feb)\n",
    "    Q_diag_feb = diags(q_diag_feb,0)\n",
    "    q_prior_array_jul =prior_array_jul.copy()\n",
    "    q_prior_array_jul[q_prior_array_jul < 1] = q_floor\n",
    "    q_diag_jul = (q_param**2) * np.square(q_prior_array_jul)\n",
    "    Q_diag_jul = diags(q_diag_jul,0)\n",
    "\n",
    "print('##')\n",
    "print('Q Feb & July scaling factor is ' + str(round(q_param**2))+ ' (umol/m2s)^2')\n",
    "print('Created Q_jul and Q_feb!')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcc412-3c37-4d63-9d5d-858e4b68d54f",
   "metadata": {},
   "source": [
    "What can you say about how your modelled enhancements are different than your observed enhancements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93404855-2441-4132-ad26-549854ffbbd5",
   "metadata": {},
   "source": [
    "Now we have to create all the pieces for the inversion for Feb and July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7a614-a263-4885-94cd-328cfcf9a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HQHt &QHt Feb and July\n",
    "HQ_feb = Hmatrix_feb@Q_diag_feb\n",
    "HQ_jul = Hmatrix_jul@Q_diag_jul\n",
    "\n",
    "def create_diagonal_matrix(vector):\n",
    "  vector_size = len(vector)\n",
    "  diagonal_matrix = np.zeros((vector_size, vector_size))\n",
    "  diagonal_matrix[np.diag_indices(vector_size)] = vector\n",
    "  return diagonal_matrix\n",
    "\n",
    "R_diagonal_Feb = create_diagonal_matrix(R_Feb)\n",
    "R_diagonal_Jul = create_diagonal_matrix(R_Jul)\n",
    "\n",
    "Htrans_feb = Hmatrix_feb.T\n",
    "HQHt_feb = HQ_feb@Htrans_feb\n",
    "QHt_feb = Q_diag_feb@Htrans_feb\n",
    "Psi_feb = HQHt_feb.toarray()+R_Feb\n",
    "Psi_inv_feb = dense_inv(Psi_feb)\n",
    "\n",
    "Htrans_jul = Hmatrix_jul.T\n",
    "HQHt_jul = HQ_jul@Htrans_jul\n",
    "QHt_jul = Q_diag_jul@Htrans_jul\n",
    "HQ_jul = Hmatrix_jul@Q_diag_jul\n",
    "Psi_jul = HQHt_jul.toarray()+R_Jul\n",
    "Psi_inv_jul = dense_inv(Psi_jul)\n",
    "\n",
    "psi_z_feb= Psi_inv_feb@(y_feb_array_truth-Hsp_feb)\n",
    "shat_feb = prior_array_feb+QHt_feb@psi_z_feb \n",
    "\n",
    "psi_z_jul= Psi_inv_jul@(y_jul_array_truth-Hsp_jul)\n",
    "shat_jul = prior_array_jul+QHt_jul@psi_z_jul \n",
    "\n",
    "print('Feb HQ rows = ' + str(HQ_feb.shape[0]) + ', HQ cols = ' + str(HQ_feb.shape[1]))\n",
    "print('July: HQ rows = ' + str(HQ_jul.shape[0]) + ', HQ cols = ' + str(HQ_jul.shape[1]))\n",
    "print('##')\n",
    "print('Feb R diagonal  = ' + str(R_diagonal_Feb.shape[0]) + ', ' + str(R_diagonal_Feb.shape[1]))\n",
    "print('July R diagonal  = ' + str(R_diagonal_Jul.shape[0]) + ', ' + str(R_diagonal_Jul.shape[1]))\n",
    "print('##')\n",
    "print('Created HQHt +R (Psi) and inv(Psi) for Feb and July!')\n",
    "print('Feb and Jul emissions shat estimates!')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8fcdf-c0ec-419c-9bd4-444fb3196828",
   "metadata": {},
   "source": [
    "Now we will calculate approximate uncertainties.  Can you look at the code and figure out why this is an approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115ceed-25a6-434b-8e73-6aed120f5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "print('Checking that matrices are positive definite and calculating unc')\n",
    "print('Takes a little while')\n",
    "import scipy.io\n",
    "from sksparse.cholmod import cholesky\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "unc_save = False\n",
    "Hshat_feb = Hmatrix_feb@shat_feb\n",
    "Hshat_jul = Hmatrix_jul@shat_jul\n",
    "\n",
    "######chi-square Feb\n",
    "y_Hshat_feb = (y_feb_array_truth-Hshat_feb)\n",
    "y_Hshat_feb=y_Hshat_feb[:,np.newaxis]\n",
    "R_diag_inv_feb = np.linalg.inv(R_diagonal_Feb)\n",
    "#R_diag_inv_feb = R_diagonal_Feb\n",
    "y_HshatTRy_Hshat_feb = y_Hshat_feb.T@R_diag_inv_feb@y_Hshat_feb\n",
    "shat_priorT_feb = (shat_feb-prior_array_feb)\n",
    "shat_priorT_feb=shat_priorT_feb[:,np.newaxis]\n",
    "\n",
    "##Estimate Uncertainty - Only Diag\n",
    "# vshat=inv(Hsp'*inv(diag(R))*Hsp+inv(Q)); % Uncertainty\n",
    "\n",
    "#Start with Feb\n",
    "#Ensure Htrans_feb is CSR for slicing\n",
    "if not isinstance(Htrans_feb, csr_matrix):\n",
    "    Htrans_feb = Htrans_feb.tocsr()\n",
    "\n",
    "q_diag = Q_diag_feb.diagonal()          # shape: (n,)\n",
    "n = Q_diag_feb.shape[0]\n",
    "m = Psi_inv_feb.shape[0]\n",
    "batch_size = 10000                      # adjust depending on memory\n",
    "\n",
    "#Transpose H and ensure CSR format\n",
    "H = Htrans_feb.T.tocsr()                # shape: (m x n)\n",
    "\n",
    "#Scale H by q_diag and convert to CSR for slicing\n",
    "H_scaled = H.multiply(q_diag).tocsr()   # still (m x n), element-wise column scaling\n",
    "\n",
    "#Compute M_feb = H_scaled @ H_scaled.T @ Psi_inv_feb in chunks\n",
    "M_feb = np.zeros((m, m))                # result is small (581 x 581)\n",
    "\n",
    "for start in range(0, n, batch_size):\n",
    "    end = min(start + batch_size, n)\n",
    "\n",
    "    #Extract dense chunk: (m x batch_size)\n",
    "    H_chunk = H_scaled[:, start:end].toarray()\n",
    "\n",
    "    #Chunk contribution to M_feb\n",
    "    M_feb += (H_chunk @ H_chunk.T) @ Psi_inv_feb\n",
    "\n",
    "#Compute diagonal of final result: vshat_diag = q_diag + diag(H.T @ M_feb @ H)\n",
    "vshat_diag_feb = np.empty(n)\n",
    "\n",
    "for start in range(0, n, batch_size):\n",
    "    end = min(start + batch_size, n)\n",
    "\n",
    "    #Extract H chunk: (m x batch_size)\n",
    "    H_chunk = H[:, start:end].toarray()\n",
    "\n",
    "    #Compute diag(HT M H) for this chunk\n",
    "    MH_chunk = M_feb @ H_chunk                  # (m x batch_size)\n",
    "    diag_chunk = np.sum(MH_chunk * H_chunk, axis=0)  # shape: (batch_size,)\n",
    "\n",
    "    #Final update\n",
    "    vshat_diag_feb[start:end] = q_diag[start:end] + diag_chunk\n",
    "\n",
    "#Setup for Jul\n",
    "y_Hshat_jul = (y_jul_array_truth-Hshat_jul)\n",
    "y_Hshat_jul=y_Hshat_jul[:,np.newaxis]\n",
    "R_diag_inv_jul = np.linalg.inv(R_diagonal_Jul)\n",
    "y_HshatTRy_Hshat_jul = y_Hshat_jul.T@R_diag_inv_jul@y_Hshat_jul\n",
    "shat_priorT_jul = (shat_jul-prior_array_jul)\n",
    "shat_priorT_jul=shat_priorT_jul[:,np.newaxis]\n",
    "\n",
    "#Ensure Htrans_jul is CSR for slicing\n",
    "if not isinstance(Htrans_jul, csr_matrix):\n",
    "    Htrans_jul = Htrans_jul.tocsr()\n",
    "\n",
    "#Setup\n",
    "q_diag = Q_diag_jul.diagonal()          # shape: (n,)\n",
    "n = Q_diag_jul.shape[0]\n",
    "m = Psi_inv_jul.shape[0]\n",
    "batch_size = 10000                      # adjust depending on memory\n",
    "\n",
    "#Transpose H and ensure CSR format\n",
    "H = Htrans_jul.T.tocsr()                # shape: (m x n)\n",
    "\n",
    "#Scale H by q_diag and convert to CSR for slicing\n",
    "H_scaled = H.multiply(q_diag).tocsr()   # still (m x n), element-wise column scaling\n",
    "\n",
    "#Compute M_jul = H_scaled @ H_scaled.T @ Psi_inv_jul in chunks\n",
    "M_jul = np.zeros((m, m))                # result is small (581 x 581)\n",
    "\n",
    "for start in range(0, n, batch_size):\n",
    "    end = min(start + batch_size, n)\n",
    "\n",
    "    # Extract dense chunk: (m x batch_size)\n",
    "    H_chunk = H_scaled[:, start:end].toarray()\n",
    "\n",
    "    # Chunk contribution to M_jul\n",
    "    M_jul += (H_chunk @ H_chunk.T) @ Psi_inv_jul\n",
    "\n",
    "#Compute diagonal of final result: vshat_diag = q_diag + diag(H.T @ M_jul @ H)\n",
    "vshat_diag_jul = np.empty(n)\n",
    "\n",
    "for start in range(0, n, batch_size):\n",
    "    end = min(start + batch_size, n)\n",
    "\n",
    "    #Extract H chunk: (m x batch_size)\n",
    "    H_chunk = H[:, start:end].toarray()\n",
    "\n",
    "    #Compute diag(HT M H) for this chunk\n",
    "    MH_chunk = M_jul @ H_chunk                  # (m x batch_size)\n",
    "    diag_chunk = np.sum(MH_chunk * H_chunk, axis=0)  # shape: (batch_size,)\n",
    "\n",
    "    vshat_diag_jul[start:end] = q_diag[start:end] + diag_chunk\n",
    "\n",
    "print('Feb and Jul emissions diag uncertainty estimated!')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2317bff-a361-4ba0-b03c-80c45b3919e2",
   "metadata": {},
   "source": [
    "Now lets look at the mean (which means that this code block is rearranging things).  But the inversion estimates hourly fluxes so you can change code to see how this looks at other time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4d99a-8744-44f1-8762-eeec91fb7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearranging shat, priors, and truths \n",
    "utctime_feb = np.load('G:/SummerSchool/output/prior/H_ACES_FFDAS_2019_02_utctimes.npy',allow_pickle=True)\n",
    "utctime_jul = np.load('G:/SummerSchool/output/prior/H_ACES_FFDAS_2019_07_utctimes.npy',allow_pickle=True)\n",
    "nhrs_back = 180\n",
    "#February\n",
    "#Shat\n",
    "reshape_shat_feb = shat_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_shat_feb = reshape_shat_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_shat_feb = trimmed_shat_feb.ravel(order='F')\n",
    "mean_shat_feb = np.mean(trimmed_shat_feb,axis = 0)\n",
    "mean_shat_feb_week = np.mean(trimmed_shat_feb, axis=1)\n",
    "#Posterior Unc Vshat\n",
    "reshape_vshat_diag_feb = vshat_diag_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_vshat_diag_feb = reshape_vshat_diag_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_vshat_diag_feb = trimmed_vshat_diag_feb.ravel(order='F')\n",
    "mean_vshat_diag_feb = np.mean(trimmed_vshat_diag_feb,axis = 0)\n",
    "#Prior Unc Q\n",
    "reshape_q_diag_feb = q_diag_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_q_diag_feb = reshape_q_diag_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_feb = trimmed_q_diag_feb.ravel(order='F')\n",
    "mean_q_diag_feb = np.mean(trimmed_q_diag_feb,axis = 0)\n",
    "\n",
    "#July\n",
    "#Shat\n",
    "reshape_shat_jul = shat_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_shat_jul = reshape_shat_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_shat_jul = trimmed_shat_jul.ravel(order='F')\n",
    "mean_shat_jul = np.mean(trimmed_shat_jul,axis = 0)\n",
    "mean_shat_jul_week = np.mean(trimmed_shat_jul, axis=1)\n",
    "#Posterior Unc Vshat\n",
    "reshape_vshat_diag_jul = vshat_diag_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_vshat_diag_jul = reshape_vshat_diag_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_vshat_diag_jul = trimmed_vshat_diag_jul.ravel(order='F')\n",
    "mean_vshat_diag_jul = np.mean(trimmed_vshat_diag_jul,axis = 0)\n",
    "#Prior Unc Q\n",
    "reshape_q_diag_jul = q_diag_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_q_diag_jul = reshape_q_diag_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_jul = trimmed_q_diag_jul.ravel(order='F')\n",
    "mean_q_diag_jul = np.mean(trimmed_q_diag_jul,axis = 0)\n",
    "\n",
    "\n",
    "mean_shat_feb = mean_shat_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_shat_jul = mean_shat_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_vshat_diag_feb = mean_vshat_diag_feb.reshape(len(lon_grid),len(lat_grid)) #\n",
    "mean_vshat_diag_jul = mean_vshat_diag_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "prior_array_feb = prior_array_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "prior_array_feb = prior_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_prior_array_feb = prior_array_feb.ravel(order='F')\n",
    "mean_prior_array_feb = np.mean(prior_array_feb,axis = 0)\n",
    "mean_prior_array_feb = mean_prior_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "prior_array_jul = prior_array_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "prior_array_jul = prior_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_prior_array_jul = prior_array_jul.ravel(order='F')\n",
    "mean_prior_array_jul = np.mean(prior_array_jul,axis = 0)\n",
    "mean_prior_array_jul = mean_prior_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "q_diag_array_feb = q_diag_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "q_diag_array_feb = q_diag_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_array_feb = q_diag_array_feb.ravel(order='F')\n",
    "mean_q_diag_array_feb = np.mean(q_diag_array_feb,axis = 0)\n",
    "mean_q_diag_array_feb = mean_q_diag_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "q_diag_array_jul = q_diag_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "q_diag_array_jul = q_diag_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_array_jul = q_diag_array_jul.ravel(order='F')\n",
    "mean_q_diag_array_jul = np.mean(q_diag_array_jul,axis = 0)\n",
    "mean_q_diag_array_jul = mean_q_diag_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "mean_prior_array_feb = mean_prior_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_prior_array_jul = mean_prior_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_q_diag_array_feb = mean_q_diag_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon a\n",
    "mean_q_diag_array_jul = mean_q_diag_array_jul.reshape(len(lon_grid),len(lat_grid))\n",
    "\n",
    "#Truth Feb and July\n",
    "truth_array_feb = truth_array_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "truth_array_feb = truth_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_truth_array_feb = truth_array_feb.ravel(order='F')\n",
    "mean_truth_array_feb = np.mean(truth_array_feb,axis = 0)\n",
    "mean_truth_array_feb = mean_truth_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "truth_array_jul = truth_array_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "truth_array_jul = truth_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_truth_array_jul = truth_array_jul.ravel(order='F')\n",
    "mean_truth_array_jul = np.mean(truth_array_jul,axis = 0)\n",
    "mean_truth_array_jul = mean_truth_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "print('Configured shat, prior emissions, emissions truth, prior Q, vshat diagonal!')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627cb4be-1f53-4af2-b642-a8acc33d1fe7",
   "metadata": {},
   "source": [
    "Let's look at the overall statistics for the full domain.  What do they tell us?  Make sure you try base case first so you make sure everything works correctly.  The code DOES NOT calculate the chi-squared statistic.  This is something for you to do later.  It is an important metric to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392e810-537e-4d3f-9d9e-a6aba9be8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Calculating Statistics\n",
    "diff_prior_feb = mean_shat_feb-mean_prior_array_feb\n",
    "diff_prior_jul = mean_shat_jul-mean_prior_array_jul\n",
    "\n",
    "diff_truth_feb = mean_shat_feb-mean_truth_array_feb\n",
    "diff_truth_jul = mean_shat_jul-mean_truth_array_jul\n",
    "\n",
    "diff_truth_feb_flat = np.sum(flatten_shat_feb - flatten_truth_array_feb)/len(flatten_shat_feb)\n",
    "diff_truth_jul_flat = np.sum(flatten_shat_jul - flatten_truth_array_jul)/len(flatten_shat_feb)\n",
    "\n",
    "meandiff_feb_flat = round(diff_truth_feb_flat ,4)\n",
    "meandiff_jul_flat = round(diff_truth_jul_flat,4)\n",
    "\n",
    "def calculate_rmse(truth, estimated):\n",
    "    assert truth.shape  == estimated.shape, \"Arrays must be the same shape!\"\n",
    "    rmse = np.sqrt(np.nanmean((truth-estimated)**2))\n",
    "    return rmse\n",
    "\n",
    "rmse_feb = calculate_rmse(flatten_shat_feb,flatten_truth_array_feb)\n",
    "rmse_jul = calculate_rmse(flatten_shat_jul,flatten_truth_array_jul)\n",
    "\n",
    "corr_matrix_feb = np.corrcoef(Hshat_feb,y_feb_array_truth_hold)\n",
    "ycorr_coef_feb = corr_matrix_feb[0,1]\n",
    "corr_matrix_jul = np.corrcoef(Hshat_jul,y_jul_array_truth_hold)\n",
    "ycorr_coef_jul = corr_matrix_jul[0,1]\n",
    "\n",
    "corr_matrix_feb = np.corrcoef(Hshat_feb,y_feb_array_truth_hold)\n",
    "corr_coef_feb = corr_matrix_feb[0,1]\n",
    "corr_matrix_jul = np.corrcoef(Hshat_jul,y_jul_array_truth_hold)\n",
    "corr_coef_jul = corr_matrix_jul[0,1]\n",
    "\n",
    "corr_matrix_feb = np.corrcoef(flatten_truth_array_feb,flatten_shat_feb)\n",
    "corr_coef_feb = corr_matrix_feb[0,1]\n",
    "corr_matrix_jul = np.corrcoef(flatten_truth_array_jul,flatten_shat_jul)\n",
    "corr_coef_jul = corr_matrix_jul[0,1]\n",
    "\n",
    "std_yerr_jul= np.std(y_jul_array_truth_hold-Hshat_jul)/np.sqrt(len(y_jul_array_truth_hold))\n",
    "std_yerr_feb= np.std(y_feb_array_truth_hold-Hshat_feb)/np.sqrt(len(y_feb_array_truth_hold))\n",
    "\n",
    "std_err_feb = np.std(flatten_shat_feb - flatten_truth_array_feb)/np.sqrt(len(flatten_truth_array_feb))\n",
    "std_err_jul = np.std(flatten_shat_jul - flatten_truth_array_jul)/np.sqrt(len(flatten_truth_array_jul))\n",
    "\n",
    "print('Statistics:')\n",
    "print('Mean difference (shat - truth) Feb = ' + str(meandiff_feb_flat)+' umol/m2s')\n",
    "print('Mean difference (shat - truth) Jul = ' + str(meandiff_jul_flat)+' umol/m2s')\n",
    "print('')\n",
    "print(\"RMSE Feb = \" + str(round(rmse_feb,4)) + ' umol/m2s')\n",
    "print(\"RMSE Jul = \" + str(round(rmse_jul,4)) + ' umol/m2s')\n",
    "print('')\n",
    "print(\"Correlation Coefficient (Hshat,y) Feb = \" + str(round(ycorr_coef_feb,4)))\n",
    "print(\"Correlation Coefficient (Hshat,y) Jul = \" + str(round(ycorr_coef_jul,4)))\n",
    "print('')\n",
    "print(\"Correlation Coefficient (shat,sprior) Feb = \" + str(round(corr_coef_feb,4)))\n",
    "print(\"Correlation Coefficient (shat,sprior) Jul = \" + str(round(corr_coef_jul,4)))\n",
    "print('')\n",
    "print(\"standard error (y) Feb = \" + str(round(std_yerr_feb,4)) +' ppm')\n",
    "print(\"standard error (y) Jul = \" + str(round(std_yerr_jul,4)) +' ppm')\n",
    "print('')\n",
    "print(\"standard error Feb = \" + str(round(std_err_feb,4)) +' umol/m2s')\n",
    "print(\"standard error Jul = \" + str(round(std_err_jul,4)) +' umol/m2s')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e2914-7f1c-4a78-b409-3fea102b3a82",
   "metadata": {},
   "source": [
    "Let's check out how things look in space across our entire domain and look at differences.  There will be 6 plots that show the estimates, priors, and truths.  IF we aren't running the perfect case, we will also have four more plots looking at differences.  We don't show the base case because there will be small spurious noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e5b92-1ce7-4891-b520-c086805fe194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Plotting\n",
    "print('Plotting - can take awhile')\n",
    "ua_pathname = 'G:/NEC/Regional/shapefiles/Census/UrbanAreas/tl_2023_us_uac20.zip'\n",
    "gdf_ua = gpd.read_file(ua_pathname)\n",
    "gdf_ua.crs = \"EPSG:4326\"\n",
    "\n",
    "# Select Baltimore areas\n",
    "gdf_baltimore = gdf_ua[gdf_ua['NAME20'] == \"Baltimore, MD\"]\n",
    "ua_pathname = 'G:/NEC/Regional/shapefiles/Census/UrbanAreas/tl_2023_us_uac20.zip'\n",
    "gdf_ua = gpd.read_file(ua_pathname)\n",
    "gdf_ua.crs = \"EPSG:4326\"\n",
    "geometry = [-76.583, 39.315417] \n",
    "point = Point(geometry) \n",
    "twr_gdf_NEB = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "geometry = [-76.685071, 39.344541]  \n",
    "point = Point(geometry) \n",
    "twr_gdf_NWB = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "geometry = [-76.675278, 39.255194]  \n",
    "point = Point(geometry) \n",
    "twr_gdf_HAL = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "\n",
    "# Get bounds\n",
    "minx, miny, maxx, maxy = gdf_baltimore.total_bounds\n",
    "bounds = gdf_baltimore.total_bounds\n",
    "mask_polygon = box(*bounds)\n",
    "\n",
    "vmin_jul = np.percentile(flatten_shat_jul, 10)\n",
    "vmax_jul = np.percentile(flatten_shat_jul, 90)\n",
    "norm_jul = Normalize(vmin=vmin_jul,vmax=vmax_jul)\n",
    "\n",
    "Plot_text = 'Estimates'\n",
    "fig, ax = plt.subplots(2,3,figsize=(16,6), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "ax[0,0].add_feature(cfeature.COASTLINE)\n",
    "ax[0,0].add_feature(cfeature.BORDERS)\n",
    "ax[0,0].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,0].pcolormesh(lon_grid,lat_grid, mean_shat_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,0].set_title('Estimates July umols/m2s' + '(truth = ' + truth + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,0])\n",
    "\n",
    "Plot_text = 'Prior'\n",
    "#mesh_grid = mean_truth_grid\n",
    "ax[0,1].add_feature(cfeature.COASTLINE)\n",
    "ax[0,1].add_feature(cfeature.BORDERS)\n",
    "ax[0,1].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,1].pcolormesh(lon_grid,lat_grid, mean_prior_array_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,1].set_title(Plot_text+' July umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,1])\n",
    "\n",
    "Plot_text = 'Truth'\n",
    "#mesh_grid = mean_truth_grid\n",
    "ax[0,2].add_feature(cfeature.COASTLINE)\n",
    "ax[0,2].add_feature(cfeature.BORDERS)\n",
    "ax[0,2].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,2].pcolormesh(lon_grid,lat_grid, mean_truth_array_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,2],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,2].set_title(Plot_text+' July umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,2])\n",
    "\n",
    "Plot_text = 'Estimates'\n",
    "vmin_feb = np.percentile(flatten_shat_feb, 10)\n",
    "vmax_feb = np.percentile(flatten_shat_feb, 90)\n",
    "norm_feb = Normalize(vmin=vmin_jul,vmax=vmax_jul)\n",
    "\n",
    "ax[1,0].add_feature(cfeature.COASTLINE)\n",
    "ax[1,0].add_feature(cfeature.BORDERS)\n",
    "ax[1,0].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,0].pcolormesh(lon_grid,lat_grid, mean_shat_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,0].set_title('Estimates Feb umols/m2s' + '(truth = ' + truth + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,0])\n",
    "\n",
    "Plot_text = 'Prior'\n",
    "ax[1,1].add_feature(cfeature.COASTLINE)\n",
    "ax[1,1].add_feature(cfeature.BORDERS)\n",
    "ax[1,1].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,1].pcolormesh(lon_grid,lat_grid, mean_prior_array_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,1].set_title(Plot_text+' Feb umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,1])\n",
    "\n",
    "Plot_text = 'Truth'\n",
    "ax[1,2].add_feature(cfeature.COASTLINE)\n",
    "ax[1,2].add_feature(cfeature.BORDERS)\n",
    "ax[1,2].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,2].pcolormesh(lon_grid,lat_grid, mean_truth_array_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,2],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,2].set_title(Plot_text+' Feb umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,2])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "plt.show() \n",
    "\n",
    "vmin_jul_prior = np.percentile(diff_prior_jul, 10)\n",
    "vmax_jul_prior = np.percentile(diff_prior_jul, 90)\n",
    "norm_jul_prior = Normalize(vmin=vmin_jul_prior,vmax=vmax_jul_prior)\n",
    "\n",
    "if not R_perfect:\n",
    "    Plot_text = '[Estimates - Prior]'\n",
    "    fig, ax = plt.subplots(2,2,figsize=(8,10), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "    ax[0,0].add_feature(cfeature.COASTLINE)\n",
    "    ax[0,0].add_feature(cfeature.BORDERS)\n",
    "    ax[0,0].add_feature(cfeature.STATES)\n",
    "    mesh = ax[0,0].pcolormesh(lon_grid,lat_grid, diff_prior_jul,cmap='viridis', norm=norm_jul_prior,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[0,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[0,0].set_title(Plot_text+' July umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[0,0])\n",
    "\n",
    "    vmin_jul_truth = np.percentile(diff_truth_jul, 10)\n",
    "    vmax_jul_truth = np.percentile(diff_truth_jul, 90)\n",
    "    norm_jul_truth = Normalize(vmin=vmin_jul_truth,vmax=vmax_jul_truth)\n",
    "    Plot_text = '[Estimates - Truth]'\n",
    "    ax[0,1].add_feature(cfeature.COASTLINE)\n",
    "    ax[0,1].add_feature(cfeature.BORDERS)\n",
    "    ax[0,1].add_feature(cfeature.STATES)\n",
    "    mesh = ax[0,1].pcolormesh(lon_grid,lat_grid, diff_truth_jul,cmap='viridis', norm=norm_jul_truth,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[0,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[0,1].set_title(Plot_text+' July umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[0,1])\n",
    "\n",
    "    vmin_feb_prior = np.percentile(diff_prior_feb, 10)\n",
    "    vmax_feb_prior = np.percentile(diff_prior_feb, 90)\n",
    "    norm_feb_prior = Normalize(vmin=vmin_feb_prior,vmax=vmax_feb_prior)\n",
    "    Plot_text = '[Estimates - Prior]'\n",
    "    ax[1,0].add_feature(cfeature.COASTLINE)\n",
    "    ax[1,0].add_feature(cfeature.BORDERS)\n",
    "    ax[1,0].add_feature(cfeature.STATES)\n",
    "    mesh = ax[1,0].pcolormesh(lon_grid,lat_grid, diff_prior_feb,cmap='viridis', norm=norm_feb_prior,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[1,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[1,0].set_title(Plot_text+' Feb umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[1,0])\n",
    "\n",
    "    vmin_feb_truth = np.percentile(diff_truth_feb, 10)\n",
    "    vmax_feb_truth = np.percentile(diff_truth_feb, 90)\n",
    "    norm_feb_truth = Normalize(vmin=vmin_feb_truth,vmax=vmax_feb_truth)\n",
    "    Plot_text = '[Estimates - Truth]'\n",
    "    ax[1,1].add_feature(cfeature.COASTLINE)\n",
    "    ax[1,1].add_feature(cfeature.BORDERS)\n",
    "    ax[1,1].add_feature(cfeature.STATES)\n",
    "    mesh = ax[1,1].pcolormesh(lon_grid,lat_grid, diff_truth_feb,cmap='viridis', norm=norm_feb_truth,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[1,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[1,1].set_title(Plot_text+ ' Feb umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[1,1])\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21899466-52bb-4b9c-831c-ff2a6d1debd6",
   "metadata": {},
   "source": [
    "Let's look at the amount of uncertainty reduced.  What do you see and why?  Remember that these are approx. uncertainties - so you will see some reduction even in the base case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5e303-285e-465e-85d1-a6c637264a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Uncertainty reduction for both Feb and July\n",
    "mesh_grid = (np.abs(mean_q_diag_array_jul - mean_vshat_diag_jul)/mean_q_diag_array_jul)*100\n",
    "vmin = np.percentile(mesh_grid.flatten(), 1)\n",
    "vmax = np.percentile(mesh_grid.flatten(), 99)\n",
    "norm = Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,10), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "\n",
    "ax[0].add_feature(cfeature.COASTLINE)\n",
    "ax[0].add_feature(cfeature.BORDERS)\n",
    "ax[0].add_feature(cfeature.STATES)\n",
    "mesh = ax[0].pcolormesh(lon_grid,lat_grid, np.sqrt(mesh_grid),cmap='viridis', norm=norm,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0].set_title('Uncertainty Reduction Jul %', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0],shrink=0.5)\n",
    "\n",
    "mesh_grid = np.abs(mean_q_diag_array_feb - mean_vshat_diag_feb)\n",
    "vmin = np.percentile(mesh_grid.flatten(), 1)\n",
    "vmax = np.percentile(mesh_grid.flatten(), 99)\n",
    "norm = Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "ax[1].add_feature(cfeature.COASTLINE)\n",
    "ax[1].add_feature(cfeature.BORDERS)\n",
    "ax[1].add_feature(cfeature.STATES)\n",
    "mesh = ax[1].pcolormesh(lon_grid,lat_grid, np.sqrt(mesh_grid),cmap='viridis', norm=norm,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1].set_title('Uncertainty Reduction Feb %', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1],shrink=0.4)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc2187-6276-4311-a7f6-4b060bcaade0",
   "metadata": {},
   "source": [
    "But what we really care about it the values inside our estimation domain (aka Baltimore). And we want to show in units that people can understand (not just the scientists!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21257fb4-826d-462b-bf03-3da8391afad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lon_2d, lat_2d = np.meshgrid(lon_grid, lat_grid)\n",
    "points = [Point(xy) for xy in zip(lon_2d.ravel(), lat_2d.ravel())]\n",
    "grid_gdf = gpd.GeoDataFrame(geometry=points, crs=gdf_baltimore.crs)\n",
    "mask_Balt = grid_gdf.within(gdf_baltimore.unary_union).values.reshape(lon_2d.shape)\n",
    "\n",
    "days_feb, days_jul = 28, 31\n",
    "sec_per_day = 86400\n",
    "\n",
    "def flux_to_GgC(mean_flux, area_grid, days):\n",
    "    mol_per_m2_s = mean_flux * 1e-6\n",
    "    g_per_m2_s = mol_per_m2_s * 12.01\n",
    "    g_per_cell_s = g_per_m2_s * area_grid\n",
    "    g_per_cell = g_per_cell_s * days * sec_per_day\n",
    "    return g_per_cell.sum() / 1e9\n",
    "\n",
    "def std_flux_to_GgC(std_flux, area_grid, days):\n",
    "    mol_per_m2_s = std_flux * 1e-6\n",
    "    g_per_m2_s = mol_per_m2_s * 12.01\n",
    "    g_per_cell_s = g_per_m2_s * area_grid\n",
    "    g_per_cell = g_per_cell_s * days * sec_per_day\n",
    "    return np.sqrt((g_per_cell**2).sum()) / 1e9\n",
    "\n",
    "#Prior mean arrays\n",
    "prior_feb = np.where(mask_Balt, mean_prior_array_feb, 0)\n",
    "prior_jul = np.where(mask_Balt, mean_prior_array_jul, 0)\n",
    "\n",
    "#Shat arrays\n",
    "post_feb = np.where(mask_Balt, mean_shat_feb, 0)\n",
    "post_jul = np.where(mask_Balt, mean_shat_jul, 0)\n",
    "\n",
    "#truth_feb = np.where(mask_Balt, mean_truth_array_feb, 0)\n",
    "#truth_jul = np.where(mask_Balt, mean_prior_array_jul, 0)\n",
    "\n",
    "#Unc arrays\n",
    "post_std_feb = np.where(mask_Balt, np.sqrt(mean_vshat_diag_feb), 0)\n",
    "post_std_jul = np.where(mask_Balt, np.sqrt(mean_vshat_diag_jul), 0)\n",
    "\n",
    "# Prior std arrays\n",
    "prior_std_feb = np.where(mask_Balt, np.sqrt(mean_q_diag_array_feb), 0)\n",
    "prior_std_jul = np.where(mask_Balt, np.sqrt(mean_q_diag_array_jul), 0)\n",
    "\n",
    "R_earth = 6.371e6\n",
    "dlat = np.radians(lat_grid[1] - lat_grid[0])  # radians\n",
    "dlon = np.radians(lon_grid[1] - lon_grid[0])  # radians\n",
    "lon_2d, lat_2d = np.meshgrid(lon_grid, lat_grid)\n",
    "area_grid = (R_earth**2) * dlat * dlon * np.cos(np.radians(lat_2d))  # in m^2\n",
    "\n",
    "prior_feb_GgC = flux_to_GgC(prior_feb, area_grid, days_feb)\n",
    "prior_jul_GgC = flux_to_GgC(prior_jul, area_grid, days_jul)\n",
    "\n",
    "post_feb_GgC = flux_to_GgC(post_feb, area_grid, days_feb)\n",
    "post_jul_GgC = flux_to_GgC(post_jul, area_grid, days_jul)\n",
    "\n",
    "#truth_feb_GgC = flux_to_GgC(truth_feb, area_grid, days_feb)\n",
    "#truth_jul_GgC = flux_to_GgC(truth_jul, area_grid, days_jul)\n",
    "\n",
    "post_unc_feb_GgC = std_flux_to_GgC(post_std_feb, area_grid, days_feb)\n",
    "post_unc_jul_GgC = std_flux_to_GgC(post_std_jul, area_grid, days_jul)\n",
    "\n",
    "prior_unc_feb_GgC = std_flux_to_GgC(prior_std_feb, area_grid, days_feb)\n",
    "prior_unc_jul_GgC = std_flux_to_GgC(prior_std_jul, area_grid, days_jul)\n",
    "\n",
    "\n",
    "labels = ['February', 'July']\n",
    "prior_vals_Balt = [prior_feb_GgC, prior_jul_GgC]\n",
    "post_vals_Balt = [post_feb_GgC, post_jul_GgC]\n",
    "#truth_vals_Balt = [truth_feb_GgC, truth_jul_GgC]\n",
    "post_uncs_Balt = [post_unc_feb_GgC, post_unc_jul_GgC]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# Prior\n",
    "ax.bar(x - width, prior_vals_Balt, width, label='Prior', color='lightgray')\n",
    "# Posterior with error bars\n",
    "ax.bar(x, post_vals_Balt, width, yerr=post_uncs_Balt*2, capsize=5, label='Posterior', color='cornflowerblue')\n",
    "# Truth\n",
    "#ax.bar(x + width, truth_vals_Balt, width, label='Truth', color='forestgreen')\n",
    "\n",
    "ax.set_ylabel('Emissions (Gg C per month)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.title('Baltimore Emissions (Prior, Posterior, Truth)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a76baa-6119-4697-bc8e-6e55095eed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% More stats for that area\n",
    "# Flatten all arrays\n",
    "flat_shat_feb = mean_shat_feb.flatten()\n",
    "flat_truth_feb = mean_truth_array_feb.flatten()\n",
    "flat_shat_jul = mean_shat_jul.flatten()\n",
    "flat_truth_jul = mean_truth_array_jul.flatten()\n",
    "\n",
    "mask_Balt_flat = mask_Balt.flatten()\n",
    "\n",
    "# Apply mask\n",
    "shat_feb_Balt = flat_shat_feb[mask_Balt_flat]\n",
    "truth_feb_Balt = flat_truth_feb[mask_Balt_flat]\n",
    "shat_jul_Balt = flat_shat_jul[mask_Balt_flat]\n",
    "truth_jul_Balt = flat_truth_jul[mask_Balt_flat]\n",
    "\n",
    "def calculate_rmse(truth, estimated):\n",
    "    return np.sqrt(np.nanmean((truth-estimated)**2))\n",
    "\n",
    "#February\n",
    "mean_diff_feb_Balt = np.mean(shat_feb_Balt - truth_feb_Balt)\n",
    "rmse_feb_Balt = calculate_rmse(truth_feb_Balt, shat_feb_Balt)\n",
    "corr_coef_feb_Balt = np.corrcoef(truth_feb_Balt, shat_feb_Balt)[0,1]\n",
    "std_err_feb_Balt = np.std(shat_feb_Balt - truth_feb_Balt)/np.sqrt(len(truth_feb_Balt))\n",
    "\n",
    "#July\n",
    "mean_diff_jul_Balt = np.mean(shat_jul_Balt - truth_jul_Balt)\n",
    "rmse_jul_Balt = calculate_rmse(truth_jul_Balt, shat_jul_Balt)\n",
    "corr_coef_jul_Balt = np.corrcoef(truth_jul_Balt, shat_jul_Balt)[0,1]\n",
    "std_err_jul_Balt = np.std(shat_jul_Balt - truth_jul_Balt)/np.sqrt(len(truth_jul_Balt))\n",
    "\n",
    "print(f\"Mean diff (shat - truth) Feb (Baltimore): {mean_diff_feb_Balt:.4f} mol/m2/s\")\n",
    "print(f\"Mean diff (shat - truth) Jul (Baltimore): {mean_diff_jul_Balt:.4f} mol/m2/s\")\n",
    "\n",
    "print(f\"RMSE Feb (Baltimore): {rmse_feb_Balt:.4f} mol/m2/s\")\n",
    "print(f\"RMSE Jul (Baltimore): {rmse_jul_Balt:.4f} mol/m2/s\")\n",
    "\n",
    "print(f\"Corr Coef Feb (Baltimore): {corr_coef_feb_Balt:.4f}\")\n",
    "print(f\"Corr Coef Jul (Baltimore): {corr_coef_jul_Balt:.4f}\")\n",
    "\n",
    "print(f\"Std Error Feb (Baltimore): {std_err_feb_Balt:.4f} mol/m2/s\")\n",
    "print(f\"Std Error Jul (Baltimore): {std_err_jul_Balt:.4f} mol/m2/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed89568-523e-495b-a928-56c3d6d48d13",
   "metadata": {},
   "source": [
    "How did these compare to what we had before for the entire domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44f3a4-ec9f-4098-ae2c-32463f1e47f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
